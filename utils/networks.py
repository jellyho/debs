from typing import Any, Optional, Sequence, Callable

import distrax
import flax.linen as nn
import jax.numpy as jnp
import flax

def default_init(scale=1.0):
    """Default kernel initializer."""
    return nn.initializers.variance_scaling(scale, 'fan_avg', 'uniform')


def ensemblize(cls, num_qs, in_axes=None, out_axes=0, **kwargs):
    """Ensemblize a module."""
    return nn.vmap(
        cls,
        variable_axes={'params': 0, 'intermediates': 0},
        split_rngs={'params': True},
        in_axes=in_axes,
        out_axes=out_axes,
        axis_size=num_qs,
        **kwargs,
    )


class FourierFeatures(nn.Module):
    # used for timestep embedding
    output_size: int = 64
    learnable: bool = False

    @nn.compact
    def __call__(self, x: jnp.ndarray):
        if self.learnable:
            w = self.param('kernel', nn.initializers.normal(0.2),
                           (self.output_size // 2, x.shape[-1]), jnp.float32)
            f = 2 * jnp.pi * x @ w.T
        else:
            half_dim = self.output_size // 2
            f = jnp.log(10000) / (half_dim - 1)
            f = jnp.exp(jnp.arange(half_dim) * -f)
            f = x * f
        return jnp.concatenate([jnp.cos(f), jnp.sin(f)], axis=-1)



class Identity(nn.Module):
    """Identity layer."""

    def __call__(self, x):
        return x

def zero_init():
    return nn.initializers.zeros

class MLP(nn.Module):
    """Multi-layer perceptron.

    Attributes:
        hidden_dims: Hidden layer dimensions.
        activations: Activation function.
        activate_final: Whether to apply activation to the final layer.
        kernel_init: Kernel initializer.
        layer_norm: Whether to apply layer normalization.
    """

    hidden_dims: Sequence[int]
    activations: Any = nn.gelu
    activate_final: bool = False
    kernel_init: Any = default_init()
    layer_norm: bool = False

    @nn.compact
    def __call__(self, x):
        for i, size in enumerate(self.hidden_dims):
            x = nn.Dense(size, kernel_init=self.kernel_init)(x)
            if i + 1 < len(self.hidden_dims) or self.activate_final:
                x = self.activations(x)
                if self.layer_norm:
                    x = nn.LayerNorm()(x)
            if i == len(self.hidden_dims) - 2:
                self.sow('intermediates', 'feature', x)
        return x

class LogParam(nn.Module):
    """Scalar parameter module with log scale."""

    init_value: float = 1.0

    @nn.compact
    def __call__(self):
        log_value = self.param('log_value', init_fn=lambda key: jnp.full((), jnp.log(self.init_value)))
        return jnp.exp(log_value)


class TransformedWithMode(distrax.Transformed):
    """Transformed distribution with mode calculation."""

    def mode(self):
        return self.bijector.forward(self.distribution.mode())


class Actor(nn.Module):
    """Gaussian actor network.

    Attributes:
        hidden_dims: Hidden layer dimensions.
        action_dim: Action dimension.
        layer_norm: Whether to apply layer normalization.
        log_std_min: Minimum value of log standard deviation.
        log_std_max: Maximum value of log standard deviation.
        tanh_squash: Whether to squash the action with tanh.
        state_dependent_std: Whether to use state-dependent standard deviation.
        const_std: Whether to use constant standard deviation.
        final_fc_init_scale: Initial scale of the final fully-connected layer.
        encoder: Optional encoder module to encode the inputs.
    """

    hidden_dims: Sequence[int]
    action_dim: int
    layer_norm: bool = False
    log_std_min: Optional[float] = -20
    log_std_max: Optional[float] = 2
    tanh_squash: bool = False
    state_dependent_std: bool = False
    const_std: bool = True
    final_fc_init_scale: float = 1e-2
    encoder: nn.Module = None

    def setup(self):
        self.actor_net = MLP(self.hidden_dims, activate_final=True, layer_norm=self.layer_norm)
        self.mean_net = nn.Dense(self.action_dim, kernel_init=default_init(self.final_fc_init_scale))
        if self.state_dependent_std:
            self.log_std_net = nn.Dense(self.action_dim, kernel_init=default_init(self.final_fc_init_scale))
        else:
            if not self.const_std:
                self.log_stds = self.param('log_stds', nn.initializers.zeros, (self.action_dim,))

    def __call__(
        self,
        observations,
        temperature=1.0,
    ):
        """Return action distributions.

        Args:
            observations: Observations.
            temperature: Scaling factor for the standard deviation.
        """
        if self.encoder is not None:
            inputs = self.encoder(observations)
        else:
            inputs = observations
        outputs = self.actor_net(inputs)

        means = self.mean_net(outputs)
        if self.state_dependent_std:
            log_stds = self.log_std_net(outputs)
        else:
            if self.const_std:
                log_stds = jnp.zeros_like(means)
            else:
                log_stds = self.log_stds

        log_stds = jnp.clip(log_stds, self.log_std_min, self.log_std_max)

        distribution = distrax.MultivariateNormalDiag(loc=means, scale_diag=jnp.exp(log_stds) * temperature)
        if self.tanh_squash:
            distribution = TransformedWithMode(distribution, distrax.Block(distrax.Tanh(), ndims=1))

        return distribution


class Value(nn.Module):
    """Value/critic network.

    This module can be used for both value V(s, g) and critic Q(s, a, g) functions.

    Attributes:
        hidden_dims: Hidden layer dimensions.
        layer_norm: Whether to apply layer normalization.
        num_ensembles: Number of ensemble components.
        encoder: Optional encoder module to encode the inputs.
    """

    hidden_dims: Sequence[int]
    layer_norm: bool = True
    num_ensembles: int = 1
    encoder: nn.Module = None

    def setup(self):
        mlp_class = MLP
        if self.num_ensembles > 1:
            mlp_class = ensemblize(mlp_class, self.num_ensembles)
        value_net = mlp_class((*self.hidden_dims, 1), activate_final=False, layer_norm=self.layer_norm)

        self.value_net = value_net

    def __call__(self, observations, actions=None):
        """Return values or critic values.

        Args:
            observations: Observations.
            actions: Actions (optional).
        """
        if self.encoder is not None:
            if isinstance(observations, (dict, flax.core.FrozenDict)):
                img_embed = self.encoder(observations['image'])
                state = observations['state']
                inputs = [img_embed, state]
            else:
                inputs = [self.encoder(observations)]
        else:
            inputs = [observations]
        if actions is not None:
            inputs.append(actions)
        inputs = jnp.concatenate(inputs, axis=-1)

        v = self.value_net(inputs).squeeze(-1)

        return v

class QuantileValue(nn.Module):
    """Quantile Value/Critic network for Distributional RL.

    Outputs a vector of quantiles Z(s) instead of a scalar mean V(s).

    Attributes:
        hidden_dims: Hidden layer dimensions.
        layer_norm: Whether to apply layer normalization.
        num_ensembles: Number of ensemble components.
        num_quantiles: The size of the output vector (N quantiles).
        encoder: Optional encoder module to encode the inputs.
    """

    hidden_dims: Sequence[int]
    layer_norm: bool = True
    num_ensembles: int = 1
    num_quantiles: int = 32  # <-- The new crucial attribute
    encoder: nn.Module = None

    def setup(self):
        mlp_class = MLP
        if self.num_ensembles > 1:
            # Ensemblizing the MLP ensures that we get multiple independent
            # sets of (num_quantiles) outputs.
            mlp_class = ensemblize(mlp_class, self.num_ensembles)
        
        # The final output dimension is set to num_quantiles, not 1.
        value_net = mlp_class(
            (*self.hidden_dims, self.num_quantiles), 
            activate_final=False, 
            layer_norm=self.layer_norm
        )
        self.value_net = value_net

    def __call__(self, observations: jnp.ndarray, actions=None) -> jnp.ndarray:
        """Return quantile values Z(s).

        Args:
            observations: Observations (s). # B, D
            
        Returns:
            Quantile tensor Z:
            - If num_ensembles=1: (Batch, num_quantiles)
            - If num_ensembles>1: (num_ensembles, Batch, num_quantiles)
        """
        if self.encoder is not None:
            inputs = [self.encoder(observations)]
        else:
            inputs = [observations]

        if actions is not None:
            inputs.append(actions)
            
        inputs = jnp.concatenate(inputs, axis=-1)

        # Output shape: (num_ensembles, Batch, num_quantiles)
        z_quantiles = self.value_net(inputs)
        
        return z_quantiles

class ActorVectorField(nn.Module):
    """Actor vector field network for flow matching.

    Attributes:
        hidden_dims: Hidden layer dimensions.
        action_dim: Action dimension.
        layer_norm: Whether to apply layer normalization.
        encoder: Optional encoder module to encode the inputs.
    """

    hidden_dims: Sequence[int]
    action_dim: int
    layer_norm: bool = False
    encoder: nn.Module = None
    use_fourier_features: bool = False
    fourier_feature_dim: int = 64
    latent_dist: str = 'normal'

    def setup(self) -> None:
        self.mlp = MLP((*self.hidden_dims, self.action_dim), activate_final=False, layer_norm=self.layer_norm)
        if self.use_fourier_features:
            self.ff = FourierFeatures(self.fourier_feature_dim)

    @nn.compact
    def __call__(self, observations, actions=None, times=None, v_base=None, is_encoded=False, rng=None):
        """Return the vectors at the given states, actions, and times (optional).

        Args:
            observations: Observations.
            actions: Actions.
            times: Times (optional).
            is_encoded: Whether the observations are already encoded.
        """
        
        inputs = []
        if self.encoder is not None:
            if isinstance(observations, (dict, flax.core.FrozenDict)):
                if not is_encoded:
                    img_embed = self.encoder(observations['image'])
                else:
                    img_embed = observations['image']
                state = observations['state']
                inputs = [img_embed, state]
            else:
                if not is_encoded:
                    inputs = [self.encoder(observations)]
                else:
                    inputs = [observations]
        else:
            inputs = [observations]

        if actions is not None:
            inputs.append(actions)

        if times is not None:
            if self.use_fourier_features:
                times = self.ff(times)
            inputs.append(times)

        if v_base is not None:
            inputs.append(v_base)

        inputs = jnp.concatenate(inputs, axis=-1)

        v = self.mlp(inputs)

        if self.latent_dist == 'normal':
            return v
        elif self.latent_dist == 'truncated_normal':
            norm = jnp.linalg.norm(v, axis=-1, keepdims=True)
            scale = 2.0 * jnp.tanh(norm) / (norm + 1e-6)
            z = v * scale
            return z
        elif self.latent_dist == 'uniform':
            return nn.tanh(v)
        elif self.latent_dist == 'simplex':
            return nn.softmax(v)
        elif self.latent_dist == 'sphere':
            sq_sum = jnp.sum(jnp.square(v), axis=-1, keepdims=True)
            norm = jnp.sqrt(sq_sum + 1e-6) 
            return v / norm * jnp.sqrt(self.action_dim)
        elif self.latent_dist == 'beta':
            return 2 * nn.tanh(v)
        return v
    
class ActorMeanFlowField(nn.Module):
    """
    Actor vector field network that conditions on an 'advantage' scalar.
    
    This network uses Fourier Features to embed the advantage value.
    Since deep networks tend to be biased towards low-frequency functions (Spectral Bias),
    directly concatenating a scalar advantage value often leads to poor conditioning.
    Projecting the bounded advantage value into a higher-dimensional Fourier space
    helps the network capture high-frequency dependencies on the advantage.

    Attributes:
        hidden_dims: Hidden layer dimensions.
        action_dim: Action dimension.
        layer_norm: Whether to apply layer normalization.
        encoder: Optional encoder module to encode the inputs.
    """

    hidden_dims: Sequence[int]
    action_dim: int
    layer_norm: bool = False
    encoder: nn.Module = None
    use_fourier_features: bool = False
    fourier_feature_dim: int = 16

    def setup(self) -> None:
        # Standard MLP for the vector field
        self.mlp = MLP((*self.hidden_dims, self.action_dim), activate_final=False, layer_norm=self.layer_norm)
        # Fourier Features for Timestep (t)
        if self.use_fourier_features:
            self.ff = FourierFeatures(self.fourier_feature_dim)

    @nn.compact
    def __call__(self, observations, actions, t, r=None, g=None, is_encoded=False):
        """
        Return the vectors at the given states, actions, advantage, and times.

        Args:
            observations: Observations (Batch, Obs_Dim).
            actions: Actions (Batch, Act_Dim).
            r: r
            t: Times (optional).
            mask: Bool (True for masking (concond), False for conditioning)
            is_encoded: Whether the observations are already encoded.
        """
        # 1. Encode observations if necessary
        inputs_list = []
        if self.encoder is not None:
            if isinstance(observations, (dict, flax.core.FrozenDict)):
                if not is_encoded:
                    img_embed = self.encoder(observations['image'])
                else:
                    img_embed = observations['image']
                state = observations['state']
                inputs_list = [img_embed, state]
            else:
                inputs_list = [self.encoder(observations)]
        else:
            inputs_list = [observations]

        # 3. Construct Input List
        inputs_list.append(actions)

        if t is not None:
            if self.use_fourier_features:
                t = self.ff(t)
            inputs_list.append(t)

        if r is not None:
            if self.use_fourier_features:
                r = self.ff(r)
            inputs_list.append(r)

        if g is not None:
            inputs_list.append(g)

        # 5. Concatenate all inputs along the last dimension
        inputs = jnp.concatenate(inputs_list, axis=-1)

        # 6. Pass through the MLP
        v = self.mlp(inputs)

        return v